{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Data Collection\n",
    "#Step 2 : Feature Extraction | Faces in this case\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 193 frames and saved 580 cropped eye images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def capture_and_crop_eyes(video_path):\n",
    "    # Load the Haar cascade for eye detection\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    # Create a folder to store the cropped eye images\n",
    "    if not os.path.exists('cropped'):\n",
    "        os.makedirs('cropped')\n",
    "    \n",
    "    frame_count = 0\n",
    "    eye_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Read a frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # If no frame is returned, the video has ended\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "        # Convert the frame to grayscale for eye detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect eyes in the frame\n",
    "        eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        # Crop and save each detected eye\n",
    "        for (x, y, w, h) in eyes:\n",
    "            eye = frame[y:y+h, x:x+w]\n",
    "            eye_count += 1\n",
    "            eye_filename = f'cropped/eye_{eye_count}.png'\n",
    "            cv2.imwrite(eye_filename, eye)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"Processed {frame_count} frames and saved {eye_count} cropped eye images.\")\n",
    "\n",
    "# Example usage:\n",
    "video_path = r'video\\WhatsApp Video 2024-07-31 at 23.12.48_43b230b9.mp4'\n",
    "capture_and_crop_eyes(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction labels:\n",
      "down: 0\n",
      "left: 1\n",
      "right: 2\n",
      "up: 3\n",
      "\n",
      "Dataset:\n",
      "dataset3\\down\\eye_465.png: 0\n",
      "dataset3\\down\\eye_466.png: 0\n",
      "dataset3\\down\\eye_468.png: 0\n",
      "dataset3\\down\\eye_472.png: 0\n",
      "dataset3\\down\\eye_473.png: 0\n",
      "dataset3\\down\\eye_474.png: 0\n",
      "dataset3\\down\\eye_477.png: 0\n",
      "dataset3\\down\\eye_479.png: 0\n",
      "dataset3\\down\\eye_480.png: 0\n",
      "dataset3\\down\\eye_481.png: 0\n",
      "dataset3\\down\\eye_482.png: 0\n",
      "dataset3\\down\\eye_483.png: 0\n",
      "dataset3\\down\\eye_484.png: 0\n",
      "dataset3\\down\\eye_485.png: 0\n",
      "dataset3\\down\\eye_486.png: 0\n",
      "dataset3\\down\\eye_487.png: 0\n",
      "dataset3\\down\\eye_488.png: 0\n",
      "dataset3\\down\\eye_489.png: 0\n",
      "dataset3\\down\\eye_490.png: 0\n",
      "dataset3\\down\\eye_491.png: 0\n",
      "dataset3\\down\\eye_492.png: 0\n",
      "dataset3\\down\\eye_493.png: 0\n",
      "dataset3\\down\\eye_494.png: 0\n",
      "dataset3\\down\\eye_495.png: 0\n",
      "dataset3\\down\\eye_496.png: 0\n",
      "dataset3\\down\\eye_497.png: 0\n",
      "dataset3\\down\\eye_498.png: 0\n",
      "dataset3\\down\\eye_499.png: 0\n",
      "dataset3\\down\\eye_500.png: 0\n",
      "dataset3\\down\\eye_501.png: 0\n",
      "dataset3\\down\\eye_502.png: 0\n",
      "dataset3\\down\\eye_503.png: 0\n",
      "dataset3\\down\\eye_504.png: 0\n",
      "dataset3\\down\\eye_505.png: 0\n",
      "dataset3\\down\\eye_506.png: 0\n",
      "dataset3\\down\\eye_507.png: 0\n",
      "dataset3\\down\\eye_508.png: 0\n",
      "dataset3\\down\\eye_509.png: 0\n",
      "dataset3\\down\\eye_510.png: 0\n",
      "dataset3\\down\\eye_511.png: 0\n",
      "dataset3\\down\\eye_512.png: 0\n",
      "dataset3\\down\\eye_513.png: 0\n",
      "dataset3\\down\\eye_514.png: 0\n",
      "dataset3\\down\\eye_516.png: 0\n",
      "dataset3\\down\\eye_517.png: 0\n",
      "dataset3\\down\\eye_518.png: 0\n",
      "dataset3\\down\\eye_519.png: 0\n",
      "dataset3\\down\\eye_520.png: 0\n",
      "dataset3\\down\\eye_521.png: 0\n",
      "dataset3\\down\\eye_522.png: 0\n",
      "dataset3\\down\\eye_523.png: 0\n",
      "dataset3\\down\\eye_524.png: 0\n",
      "dataset3\\down\\eye_525.png: 0\n",
      "dataset3\\down\\eye_526.png: 0\n",
      "dataset3\\down\\eye_527.png: 0\n",
      "dataset3\\down\\eye_528.png: 0\n",
      "dataset3\\down\\eye_529.png: 0\n",
      "dataset3\\down\\eye_530.png: 0\n",
      "dataset3\\down\\eye_531.png: 0\n",
      "dataset3\\down\\eye_532.png: 0\n",
      "dataset3\\down\\eye_533.png: 0\n",
      "dataset3\\down\\eye_534.png: 0\n",
      "dataset3\\down\\eye_535.png: 0\n",
      "dataset3\\down\\eye_536.png: 0\n",
      "dataset3\\down\\eye_537.png: 0\n",
      "dataset3\\down\\eye_538.png: 0\n",
      "dataset3\\down\\eye_539.png: 0\n",
      "dataset3\\down\\eye_540.png: 0\n",
      "dataset3\\down\\eye_541.png: 0\n",
      "dataset3\\down\\eye_542.png: 0\n",
      "dataset3\\down\\eye_543.png: 0\n",
      "dataset3\\down\\eye_544.png: 0\n",
      "dataset3\\down\\eye_545.png: 0\n",
      "dataset3\\down\\eye_546.png: 0\n",
      "dataset3\\down\\eye_547.png: 0\n",
      "dataset3\\down\\eye_548.png: 0\n",
      "dataset3\\down\\eye_549.png: 0\n",
      "dataset3\\down\\eye_550.png: 0\n",
      "dataset3\\down\\eye_551.png: 0\n",
      "dataset3\\down\\eye_552.png: 0\n",
      "dataset3\\down\\eye_553.png: 0\n",
      "dataset3\\down\\eye_554.png: 0\n",
      "dataset3\\down\\eye_555.png: 0\n",
      "dataset3\\down\\eye_556.png: 0\n",
      "dataset3\\down\\eye_557.png: 0\n",
      "dataset3\\down\\eye_558.png: 0\n",
      "dataset3\\down\\eye_559.png: 0\n",
      "dataset3\\down\\eye_560.png: 0\n",
      "dataset3\\down\\eye_561.png: 0\n",
      "dataset3\\down\\eye_562.png: 0\n",
      "dataset3\\down\\eye_563.png: 0\n",
      "dataset3\\down\\eye_564.png: 0\n",
      "dataset3\\down\\eye_565.png: 0\n",
      "dataset3\\down\\eye_566.png: 0\n",
      "dataset3\\down\\eye_567.png: 0\n",
      "dataset3\\down\\eye_568.png: 0\n",
      "dataset3\\down\\eye_569.png: 0\n",
      "dataset3\\down\\eye_570.png: 0\n",
      "dataset3\\down\\eye_571.png: 0\n",
      "dataset3\\down\\eye_572.png: 0\n",
      "dataset3\\down\\eye_573.png: 0\n",
      "dataset3\\down\\eye_574.png: 0\n",
      "dataset3\\down\\eye_575.png: 0\n",
      "dataset3\\down\\eye_576.png: 0\n",
      "dataset3\\down\\eye_577.png: 0\n",
      "dataset3\\down\\eye_578.png: 0\n",
      "dataset3\\down\\eye_579.png: 0\n",
      "dataset3\\down\\eye_580.png: 0\n",
      "dataset3\\left\\eye_1.png: 1\n",
      "dataset3\\left\\eye_10.png: 1\n",
      "dataset3\\left\\eye_11.png: 1\n",
      "dataset3\\left\\eye_12.png: 1\n",
      "dataset3\\left\\eye_13.png: 1\n",
      "dataset3\\left\\eye_14.png: 1\n",
      "dataset3\\left\\eye_15.png: 1\n",
      "dataset3\\left\\eye_16.png: 1\n",
      "dataset3\\left\\eye_17.png: 1\n",
      "dataset3\\left\\eye_18.png: 1\n",
      "dataset3\\left\\eye_19.png: 1\n",
      "dataset3\\left\\eye_2.png: 1\n",
      "dataset3\\left\\eye_20.png: 1\n",
      "dataset3\\left\\eye_21.png: 1\n",
      "dataset3\\left\\eye_22.png: 1\n",
      "dataset3\\left\\eye_23.png: 1\n",
      "dataset3\\left\\eye_24.png: 1\n",
      "dataset3\\left\\eye_25.png: 1\n",
      "dataset3\\left\\eye_26.png: 1\n",
      "dataset3\\left\\eye_27.png: 1\n",
      "dataset3\\left\\eye_28.png: 1\n",
      "dataset3\\left\\eye_29.png: 1\n",
      "dataset3\\left\\eye_3.png: 1\n",
      "dataset3\\left\\eye_30.png: 1\n",
      "dataset3\\left\\eye_31.png: 1\n",
      "dataset3\\left\\eye_32.png: 1\n",
      "dataset3\\left\\eye_33.png: 1\n",
      "dataset3\\left\\eye_34.png: 1\n",
      "dataset3\\left\\eye_35.png: 1\n",
      "dataset3\\left\\eye_36.png: 1\n",
      "dataset3\\left\\eye_37.png: 1\n",
      "dataset3\\left\\eye_38.png: 1\n",
      "dataset3\\left\\eye_39.png: 1\n",
      "dataset3\\left\\eye_4.png: 1\n",
      "dataset3\\left\\eye_40.png: 1\n",
      "dataset3\\left\\eye_41.png: 1\n",
      "dataset3\\left\\eye_42.png: 1\n",
      "dataset3\\left\\eye_43.png: 1\n",
      "dataset3\\left\\eye_44.png: 1\n",
      "dataset3\\left\\eye_45.png: 1\n",
      "dataset3\\left\\eye_46.png: 1\n",
      "dataset3\\left\\eye_47.png: 1\n",
      "dataset3\\left\\eye_48.png: 1\n",
      "dataset3\\left\\eye_49.png: 1\n",
      "dataset3\\left\\eye_5.png: 1\n",
      "dataset3\\left\\eye_50.png: 1\n",
      "dataset3\\left\\eye_51.png: 1\n",
      "dataset3\\left\\eye_52.png: 1\n",
      "dataset3\\left\\eye_53.png: 1\n",
      "dataset3\\left\\eye_54.png: 1\n",
      "dataset3\\left\\eye_55.png: 1\n",
      "dataset3\\left\\eye_56.png: 1\n",
      "dataset3\\left\\eye_57.png: 1\n",
      "dataset3\\left\\eye_58.png: 1\n",
      "dataset3\\left\\eye_59.png: 1\n",
      "dataset3\\left\\eye_6.png: 1\n",
      "dataset3\\left\\eye_60.png: 1\n",
      "dataset3\\left\\eye_61.png: 1\n",
      "dataset3\\left\\eye_62.png: 1\n",
      "dataset3\\left\\eye_63.png: 1\n",
      "dataset3\\left\\eye_64.png: 1\n",
      "dataset3\\left\\eye_65.png: 1\n",
      "dataset3\\left\\eye_66.png: 1\n",
      "dataset3\\left\\eye_67.png: 1\n",
      "dataset3\\left\\eye_68.png: 1\n",
      "dataset3\\left\\eye_69.png: 1\n",
      "dataset3\\left\\eye_7.png: 1\n",
      "dataset3\\left\\eye_70.png: 1\n",
      "dataset3\\left\\eye_71.png: 1\n",
      "dataset3\\left\\eye_72.png: 1\n",
      "dataset3\\left\\eye_73.png: 1\n",
      "dataset3\\left\\eye_75.png: 1\n",
      "dataset3\\left\\eye_76.png: 1\n",
      "dataset3\\left\\eye_77.png: 1\n",
      "dataset3\\left\\eye_79.png: 1\n",
      "dataset3\\left\\eye_8.png: 1\n",
      "dataset3\\left\\eye_80.png: 1\n",
      "dataset3\\left\\eye_83.png: 1\n",
      "dataset3\\left\\eye_9.png: 1\n",
      "dataset3\\right\\eye_101.png: 2\n",
      "dataset3\\right\\eye_102.png: 2\n",
      "dataset3\\right\\eye_103.png: 2\n",
      "dataset3\\right\\eye_104.png: 2\n",
      "dataset3\\right\\eye_105.png: 2\n",
      "dataset3\\right\\eye_106.png: 2\n",
      "dataset3\\right\\eye_107.png: 2\n",
      "dataset3\\right\\eye_108.png: 2\n",
      "dataset3\\right\\eye_109.png: 2\n",
      "dataset3\\right\\eye_110.png: 2\n",
      "dataset3\\right\\eye_111.png: 2\n",
      "dataset3\\right\\eye_112.png: 2\n",
      "dataset3\\right\\eye_113.png: 2\n",
      "dataset3\\right\\eye_114.png: 2\n",
      "dataset3\\right\\eye_115.png: 2\n",
      "dataset3\\right\\eye_116.png: 2\n",
      "dataset3\\right\\eye_117.png: 2\n",
      "dataset3\\right\\eye_118.png: 2\n",
      "dataset3\\right\\eye_119.png: 2\n",
      "dataset3\\right\\eye_120.png: 2\n",
      "dataset3\\right\\eye_121.png: 2\n",
      "dataset3\\right\\eye_122.png: 2\n",
      "dataset3\\right\\eye_123.png: 2\n",
      "dataset3\\right\\eye_124.png: 2\n",
      "dataset3\\right\\eye_125.png: 2\n",
      "dataset3\\right\\eye_126.png: 2\n",
      "dataset3\\right\\eye_127.png: 2\n",
      "dataset3\\right\\eye_128.png: 2\n",
      "dataset3\\right\\eye_129.png: 2\n",
      "dataset3\\right\\eye_130.png: 2\n",
      "dataset3\\right\\eye_131.png: 2\n",
      "dataset3\\right\\eye_132.png: 2\n",
      "dataset3\\right\\eye_133.png: 2\n",
      "dataset3\\right\\eye_134.png: 2\n",
      "dataset3\\right\\eye_135.png: 2\n",
      "dataset3\\right\\eye_136.png: 2\n",
      "dataset3\\right\\eye_137.png: 2\n",
      "dataset3\\right\\eye_138.png: 2\n",
      "dataset3\\right\\eye_139.png: 2\n",
      "dataset3\\right\\eye_140.png: 2\n",
      "dataset3\\right\\eye_141.png: 2\n",
      "dataset3\\right\\eye_142.png: 2\n",
      "dataset3\\right\\eye_143.png: 2\n",
      "dataset3\\right\\eye_144.png: 2\n",
      "dataset3\\right\\eye_145.png: 2\n",
      "dataset3\\right\\eye_146.png: 2\n",
      "dataset3\\right\\eye_147.png: 2\n",
      "dataset3\\right\\eye_148.png: 2\n",
      "dataset3\\right\\eye_149.png: 2\n",
      "dataset3\\right\\eye_150.png: 2\n",
      "dataset3\\right\\eye_151.png: 2\n",
      "dataset3\\right\\eye_152.png: 2\n",
      "dataset3\\right\\eye_153.png: 2\n",
      "dataset3\\right\\eye_154.png: 2\n",
      "dataset3\\right\\eye_155.png: 2\n",
      "dataset3\\right\\eye_156.png: 2\n",
      "dataset3\\right\\eye_157.png: 2\n",
      "dataset3\\right\\eye_158.png: 2\n",
      "dataset3\\right\\eye_159.png: 2\n",
      "dataset3\\right\\eye_160.png: 2\n",
      "dataset3\\right\\eye_161.png: 2\n",
      "dataset3\\right\\eye_162.png: 2\n",
      "dataset3\\right\\eye_163.png: 2\n",
      "dataset3\\right\\eye_164.png: 2\n",
      "dataset3\\right\\eye_165.png: 2\n",
      "dataset3\\right\\eye_166.png: 2\n",
      "dataset3\\right\\eye_167.png: 2\n",
      "dataset3\\right\\eye_168.png: 2\n",
      "dataset3\\right\\eye_169.png: 2\n",
      "dataset3\\right\\eye_170.png: 2\n",
      "dataset3\\right\\eye_171.png: 2\n",
      "dataset3\\right\\eye_172.png: 2\n",
      "dataset3\\right\\eye_173.png: 2\n",
      "dataset3\\right\\eye_174.png: 2\n",
      "dataset3\\right\\eye_175.png: 2\n",
      "dataset3\\right\\eye_176.png: 2\n",
      "dataset3\\right\\eye_177.png: 2\n",
      "dataset3\\right\\eye_178.png: 2\n",
      "dataset3\\right\\eye_179.png: 2\n",
      "dataset3\\right\\eye_180.png: 2\n",
      "dataset3\\right\\eye_181.png: 2\n",
      "dataset3\\right\\eye_182.png: 2\n",
      "dataset3\\right\\eye_183.png: 2\n",
      "dataset3\\right\\eye_184.png: 2\n",
      "dataset3\\right\\eye_185.png: 2\n",
      "dataset3\\right\\eye_186.png: 2\n",
      "dataset3\\right\\eye_187.png: 2\n",
      "dataset3\\right\\eye_188.png: 2\n",
      "dataset3\\right\\eye_189.png: 2\n",
      "dataset3\\right\\eye_190.png: 2\n",
      "dataset3\\right\\eye_191.png: 2\n",
      "dataset3\\right\\eye_192.png: 2\n",
      "dataset3\\right\\eye_193.png: 2\n",
      "dataset3\\right\\eye_194.png: 2\n",
      "dataset3\\right\\eye_195.png: 2\n",
      "dataset3\\right\\eye_196.png: 2\n",
      "dataset3\\right\\eye_197.png: 2\n",
      "dataset3\\right\\eye_198.png: 2\n",
      "dataset3\\right\\eye_199.png: 2\n",
      "dataset3\\right\\eye_200.png: 2\n",
      "dataset3\\right\\eye_201.png: 2\n",
      "dataset3\\right\\eye_202.png: 2\n",
      "dataset3\\right\\eye_203.png: 2\n",
      "dataset3\\right\\eye_204.png: 2\n",
      "dataset3\\right\\eye_205.png: 2\n",
      "dataset3\\right\\eye_206.png: 2\n",
      "dataset3\\right\\eye_207.png: 2\n",
      "dataset3\\right\\eye_208.png: 2\n",
      "dataset3\\right\\eye_209.png: 2\n",
      "dataset3\\right\\eye_210.png: 2\n",
      "dataset3\\right\\eye_211.png: 2\n",
      "dataset3\\right\\eye_212.png: 2\n",
      "dataset3\\right\\eye_213.png: 2\n",
      "dataset3\\right\\eye_214.png: 2\n",
      "dataset3\\right\\eye_215.png: 2\n",
      "dataset3\\right\\eye_216.png: 2\n",
      "dataset3\\right\\eye_217.png: 2\n",
      "dataset3\\right\\eye_218.png: 2\n",
      "dataset3\\right\\eye_219.png: 2\n",
      "dataset3\\right\\eye_220.png: 2\n",
      "dataset3\\right\\eye_221.png: 2\n",
      "dataset3\\right\\eye_222.png: 2\n",
      "dataset3\\right\\eye_225.png: 2\n",
      "dataset3\\right\\eye_226.png: 2\n",
      "dataset3\\right\\eye_85.png: 2\n",
      "dataset3\\right\\eye_86.png: 2\n",
      "dataset3\\right\\eye_87.png: 2\n",
      "dataset3\\right\\eye_89.png: 2\n",
      "dataset3\\right\\eye_90.png: 2\n",
      "dataset3\\right\\eye_92.png: 2\n",
      "dataset3\\right\\eye_93.png: 2\n",
      "dataset3\\right\\eye_94.png: 2\n",
      "dataset3\\right\\eye_95.png: 2\n",
      "dataset3\\right\\eye_98.png: 2\n",
      "dataset3\\right\\eye_99.png: 2\n",
      "dataset3\\up\\eye_100.png: 3\n",
      "dataset3\\up\\eye_223.png: 3\n",
      "dataset3\\up\\eye_224.png: 3\n",
      "dataset3\\up\\eye_227.png: 3\n",
      "dataset3\\up\\eye_228.png: 3\n",
      "dataset3\\up\\eye_229.png: 3\n",
      "dataset3\\up\\eye_230.png: 3\n",
      "dataset3\\up\\eye_231.png: 3\n",
      "dataset3\\up\\eye_232.png: 3\n",
      "dataset3\\up\\eye_233.png: 3\n",
      "dataset3\\up\\eye_234.png: 3\n",
      "dataset3\\up\\eye_235.png: 3\n",
      "dataset3\\up\\eye_236.png: 3\n",
      "dataset3\\up\\eye_237.png: 3\n",
      "dataset3\\up\\eye_238.png: 3\n",
      "dataset3\\up\\eye_239.png: 3\n",
      "dataset3\\up\\eye_240.png: 3\n",
      "dataset3\\up\\eye_241.png: 3\n",
      "dataset3\\up\\eye_242.png: 3\n",
      "dataset3\\up\\eye_243.png: 3\n",
      "dataset3\\up\\eye_244.png: 3\n",
      "dataset3\\up\\eye_245.png: 3\n",
      "dataset3\\up\\eye_246.png: 3\n",
      "dataset3\\up\\eye_247.png: 3\n",
      "dataset3\\up\\eye_248.png: 3\n",
      "dataset3\\up\\eye_249.png: 3\n",
      "dataset3\\up\\eye_250.png: 3\n",
      "dataset3\\up\\eye_251.png: 3\n",
      "dataset3\\up\\eye_252.png: 3\n",
      "dataset3\\up\\eye_253.png: 3\n",
      "dataset3\\up\\eye_254.png: 3\n",
      "dataset3\\up\\eye_255.png: 3\n",
      "dataset3\\up\\eye_256.png: 3\n",
      "dataset3\\up\\eye_257.png: 3\n",
      "dataset3\\up\\eye_258.png: 3\n",
      "dataset3\\up\\eye_259.png: 3\n",
      "dataset3\\up\\eye_260.png: 3\n",
      "dataset3\\up\\eye_261.png: 3\n",
      "dataset3\\up\\eye_262.png: 3\n",
      "dataset3\\up\\eye_263.png: 3\n",
      "dataset3\\up\\eye_264.png: 3\n",
      "dataset3\\up\\eye_265.png: 3\n",
      "dataset3\\up\\eye_266.png: 3\n",
      "dataset3\\up\\eye_267.png: 3\n",
      "dataset3\\up\\eye_268.png: 3\n",
      "dataset3\\up\\eye_269.png: 3\n",
      "dataset3\\up\\eye_270.png: 3\n",
      "dataset3\\up\\eye_271.png: 3\n",
      "dataset3\\up\\eye_272.png: 3\n",
      "dataset3\\up\\eye_273.png: 3\n",
      "dataset3\\up\\eye_274.png: 3\n",
      "dataset3\\up\\eye_275.png: 3\n",
      "dataset3\\up\\eye_276.png: 3\n",
      "dataset3\\up\\eye_277.png: 3\n",
      "dataset3\\up\\eye_278.png: 3\n",
      "dataset3\\up\\eye_279.png: 3\n",
      "dataset3\\up\\eye_280.png: 3\n",
      "dataset3\\up\\eye_281.png: 3\n",
      "dataset3\\up\\eye_282.png: 3\n",
      "dataset3\\up\\eye_283.png: 3\n",
      "dataset3\\up\\eye_284.png: 3\n",
      "dataset3\\up\\eye_285.png: 3\n",
      "dataset3\\up\\eye_286.png: 3\n",
      "dataset3\\up\\eye_287.png: 3\n",
      "dataset3\\up\\eye_288.png: 3\n",
      "dataset3\\up\\eye_289.png: 3\n",
      "dataset3\\up\\eye_290.png: 3\n",
      "dataset3\\up\\eye_291.png: 3\n",
      "dataset3\\up\\eye_292.png: 3\n",
      "dataset3\\up\\eye_293.png: 3\n",
      "dataset3\\up\\eye_294.png: 3\n",
      "dataset3\\up\\eye_295.png: 3\n",
      "dataset3\\up\\eye_296.png: 3\n",
      "dataset3\\up\\eye_297.png: 3\n",
      "dataset3\\up\\eye_298.png: 3\n",
      "dataset3\\up\\eye_299.png: 3\n",
      "dataset3\\up\\eye_300.png: 3\n",
      "dataset3\\up\\eye_301.png: 3\n",
      "dataset3\\up\\eye_302.png: 3\n",
      "dataset3\\up\\eye_303.png: 3\n",
      "dataset3\\up\\eye_304.png: 3\n",
      "dataset3\\up\\eye_305.png: 3\n",
      "dataset3\\up\\eye_306.png: 3\n",
      "dataset3\\up\\eye_307.png: 3\n",
      "dataset3\\up\\eye_308.png: 3\n",
      "dataset3\\up\\eye_309.png: 3\n",
      "dataset3\\up\\eye_310.png: 3\n",
      "dataset3\\up\\eye_311.png: 3\n",
      "dataset3\\up\\eye_312.png: 3\n",
      "dataset3\\up\\eye_313.png: 3\n",
      "dataset3\\up\\eye_314.png: 3\n",
      "dataset3\\up\\eye_315.png: 3\n",
      "dataset3\\up\\eye_316.png: 3\n",
      "dataset3\\up\\eye_317.png: 3\n",
      "dataset3\\up\\eye_318.png: 3\n",
      "dataset3\\up\\eye_319.png: 3\n",
      "dataset3\\up\\eye_320.png: 3\n",
      "dataset3\\up\\eye_321.png: 3\n",
      "dataset3\\up\\eye_322.png: 3\n",
      "dataset3\\up\\eye_323.png: 3\n",
      "dataset3\\up\\eye_324.png: 3\n",
      "dataset3\\up\\eye_325.png: 3\n",
      "dataset3\\up\\eye_326.png: 3\n",
      "dataset3\\up\\eye_327.png: 3\n",
      "dataset3\\up\\eye_328.png: 3\n",
      "dataset3\\up\\eye_329.png: 3\n",
      "dataset3\\up\\eye_330.png: 3\n",
      "dataset3\\up\\eye_331.png: 3\n",
      "dataset3\\up\\eye_332.png: 3\n",
      "dataset3\\up\\eye_333.png: 3\n",
      "dataset3\\up\\eye_334.png: 3\n",
      "dataset3\\up\\eye_335.png: 3\n",
      "dataset3\\up\\eye_336.png: 3\n",
      "dataset3\\up\\eye_337.png: 3\n",
      "dataset3\\up\\eye_338.png: 3\n",
      "dataset3\\up\\eye_339.png: 3\n",
      "dataset3\\up\\eye_340.png: 3\n",
      "dataset3\\up\\eye_341.png: 3\n",
      "dataset3\\up\\eye_342.png: 3\n",
      "dataset3\\up\\eye_343.png: 3\n",
      "dataset3\\up\\eye_344.png: 3\n",
      "dataset3\\up\\eye_345.png: 3\n",
      "dataset3\\up\\eye_346.png: 3\n",
      "dataset3\\up\\eye_347.png: 3\n",
      "dataset3\\up\\eye_348.png: 3\n",
      "dataset3\\up\\eye_349.png: 3\n",
      "dataset3\\up\\eye_350.png: 3\n",
      "dataset3\\up\\eye_351.png: 3\n",
      "dataset3\\up\\eye_352.png: 3\n",
      "dataset3\\up\\eye_353.png: 3\n",
      "dataset3\\up\\eye_354.png: 3\n",
      "dataset3\\up\\eye_355.png: 3\n",
      "dataset3\\up\\eye_356.png: 3\n",
      "dataset3\\up\\eye_357.png: 3\n",
      "dataset3\\up\\eye_358.png: 3\n",
      "dataset3\\up\\eye_359.png: 3\n",
      "dataset3\\up\\eye_360.png: 3\n",
      "dataset3\\up\\eye_361.png: 3\n",
      "dataset3\\up\\eye_362.png: 3\n",
      "dataset3\\up\\eye_363.png: 3\n",
      "dataset3\\up\\eye_364.png: 3\n",
      "dataset3\\up\\eye_365.png: 3\n",
      "dataset3\\up\\eye_366.png: 3\n",
      "dataset3\\up\\eye_367.png: 3\n",
      "dataset3\\up\\eye_368.png: 3\n",
      "dataset3\\up\\eye_369.png: 3\n",
      "dataset3\\up\\eye_370.png: 3\n",
      "dataset3\\up\\eye_371.png: 3\n",
      "dataset3\\up\\eye_372.png: 3\n",
      "dataset3\\up\\eye_373.png: 3\n",
      "dataset3\\up\\eye_374.png: 3\n",
      "dataset3\\up\\eye_375.png: 3\n",
      "dataset3\\up\\eye_376.png: 3\n",
      "dataset3\\up\\eye_377.png: 3\n",
      "dataset3\\up\\eye_378.png: 3\n",
      "dataset3\\up\\eye_379.png: 3\n",
      "dataset3\\up\\eye_380.png: 3\n",
      "dataset3\\up\\eye_381.png: 3\n",
      "dataset3\\up\\eye_382.png: 3\n",
      "dataset3\\up\\eye_383.png: 3\n",
      "dataset3\\up\\eye_384.png: 3\n",
      "dataset3\\up\\eye_385.png: 3\n",
      "dataset3\\up\\eye_386.png: 3\n",
      "dataset3\\up\\eye_387.png: 3\n",
      "dataset3\\up\\eye_388.png: 3\n",
      "dataset3\\up\\eye_389.png: 3\n",
      "dataset3\\up\\eye_390.png: 3\n",
      "dataset3\\up\\eye_391.png: 3\n",
      "dataset3\\up\\eye_392.png: 3\n",
      "dataset3\\up\\eye_393.png: 3\n",
      "dataset3\\up\\eye_394.png: 3\n",
      "dataset3\\up\\eye_395.png: 3\n",
      "dataset3\\up\\eye_396.png: 3\n",
      "dataset3\\up\\eye_397.png: 3\n",
      "dataset3\\up\\eye_398.png: 3\n",
      "dataset3\\up\\eye_399.png: 3\n",
      "dataset3\\up\\eye_400.png: 3\n",
      "dataset3\\up\\eye_401.png: 3\n",
      "dataset3\\up\\eye_402.png: 3\n",
      "dataset3\\up\\eye_403.png: 3\n",
      "dataset3\\up\\eye_404.png: 3\n",
      "dataset3\\up\\eye_405.png: 3\n",
      "dataset3\\up\\eye_406.png: 3\n",
      "dataset3\\up\\eye_407.png: 3\n",
      "dataset3\\up\\eye_408.png: 3\n",
      "dataset3\\up\\eye_409.png: 3\n",
      "dataset3\\up\\eye_410.png: 3\n",
      "dataset3\\up\\eye_411.png: 3\n",
      "dataset3\\up\\eye_412.png: 3\n",
      "dataset3\\up\\eye_413.png: 3\n",
      "dataset3\\up\\eye_414.png: 3\n",
      "dataset3\\up\\eye_415.png: 3\n",
      "dataset3\\up\\eye_416.png: 3\n",
      "dataset3\\up\\eye_417.png: 3\n",
      "dataset3\\up\\eye_418.png: 3\n",
      "dataset3\\up\\eye_419.png: 3\n",
      "dataset3\\up\\eye_420.png: 3\n",
      "dataset3\\up\\eye_421.png: 3\n",
      "dataset3\\up\\eye_422.png: 3\n",
      "dataset3\\up\\eye_423.png: 3\n",
      "dataset3\\up\\eye_424.png: 3\n",
      "dataset3\\up\\eye_425.png: 3\n",
      "dataset3\\up\\eye_426.png: 3\n",
      "dataset3\\up\\eye_427.png: 3\n",
      "dataset3\\up\\eye_428.png: 3\n",
      "dataset3\\up\\eye_429.png: 3\n",
      "dataset3\\up\\eye_430.png: 3\n",
      "dataset3\\up\\eye_431.png: 3\n",
      "dataset3\\up\\eye_432.png: 3\n",
      "dataset3\\up\\eye_433.png: 3\n",
      "dataset3\\up\\eye_434.png: 3\n",
      "dataset3\\up\\eye_435.png: 3\n",
      "dataset3\\up\\eye_436.png: 3\n",
      "dataset3\\up\\eye_437.png: 3\n",
      "dataset3\\up\\eye_438.png: 3\n",
      "dataset3\\up\\eye_439.png: 3\n",
      "dataset3\\up\\eye_440.png: 3\n",
      "dataset3\\up\\eye_441.png: 3\n",
      "dataset3\\up\\eye_442.png: 3\n",
      "dataset3\\up\\eye_443.png: 3\n",
      "dataset3\\up\\eye_444.png: 3\n",
      "dataset3\\up\\eye_445.png: 3\n",
      "dataset3\\up\\eye_446.png: 3\n",
      "dataset3\\up\\eye_447.png: 3\n",
      "dataset3\\up\\eye_448.png: 3\n",
      "dataset3\\up\\eye_449.png: 3\n",
      "dataset3\\up\\eye_450.png: 3\n",
      "dataset3\\up\\eye_451.png: 3\n",
      "dataset3\\up\\eye_452.png: 3\n",
      "dataset3\\up\\eye_453.png: 3\n",
      "dataset3\\up\\eye_454.png: 3\n",
      "dataset3\\up\\eye_455.png: 3\n",
      "dataset3\\up\\eye_456.png: 3\n",
      "dataset3\\up\\eye_457.png: 3\n",
      "dataset3\\up\\eye_458.png: 3\n",
      "dataset3\\up\\eye_459.png: 3\n",
      "dataset3\\up\\eye_460.png: 3\n",
      "dataset3\\up\\eye_461.png: 3\n",
      "dataset3\\up\\eye_462.png: 3\n",
      "dataset3\\up\\eye_463.png: 3\n",
      "dataset3\\up\\eye_464.png: 3\n",
      "dataset3\\up\\eye_467.png: 3\n",
      "dataset3\\up\\eye_469.png: 3\n",
      "dataset3\\up\\eye_470.png: 3\n",
      "dataset3\\up\\eye_471.png: 3\n",
      "dataset3\\up\\eye_475.png: 3\n",
      "dataset3\\up\\eye_476.png: 3\n",
      "dataset3\\up\\eye_478.png: 3\n",
      "dataset3\\up\\eye_515.png: 3\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_mapping(dataset_path):\n",
    "    direction_folders = ['down', 'left', 'right', 'up']\n",
    "    direction_labels = {direction: idx for idx, direction in enumerate(direction_folders)}\n",
    "    \n",
    "    print(\"Direction labels:\")\n",
    "    for direction, label in direction_labels.items():\n",
    "        print(f\"{direction}: {label}\")\n",
    "    \n",
    "    # Create a list to store the image paths and their corresponding labels\n",
    "    dataset = []\n",
    "\n",
    "    for direction in direction_folders:\n",
    "        folder_path = os.path.join(dataset_path, direction)\n",
    "        if os.path.exists(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".png\"):  # or any other image format\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    dataset.append((file_path, direction_labels[direction]))\n",
    "\n",
    "    return dataset, direction_labels\n",
    "\n",
    "# Example usage:\n",
    "dataset_path = 'dataset3'\n",
    "dataset, direction_labels = create_dataset_mapping(dataset_path)\n",
    "\n",
    "# Print the dataset\n",
    "print(\"\\nDataset:\")\n",
    "for file_path, label in dataset:\n",
    "    print(f\"{file_path}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2d(img):\n",
    "    imArray = img\n",
    "    imArray = cv2.cvtColor( imArray,cv2.COLOR_RGB2GRAY )\n",
    "    \n",
    "    imArray =  np.float32(imArray)   \n",
    "    imArray /= 255\n",
    "  \n",
    "\n",
    "    return imArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 456\n",
      "Validation set size: 115\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_ratio = 0.8  # 80% training, 20% validation\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "val_data = dataset[train_size:]\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (456, 1024), y_train shape: (456,)\n",
      "X_val shape: (115, 1024), y_val shape: (115,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Example function to extract features (e.g., resizing and flattening)\n",
    "def extract_features(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (32, 32))  # Resize image to (32, 32)\n",
    "    features = img.flatten()  # Flatten image into a 1D array\n",
    "    return features\n",
    "\n",
    "# Example usage to extract features from dataset\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for file_path, label in train_data:\n",
    "    features = extract_features(file_path)\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Similarly, extract features for validation data (val_data)\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for file_path, label in val_data:\n",
    "    features = extract_features(file_path)\n",
    "    X_val.append(features)\n",
    "    y_val.append(label)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9565217391304348\n",
      "Logistic Regression Accuracy: 0.9304347826086956\n",
      "Random Forest Accuracy: 0.9391304347826087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example SVM training\n",
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_pred = svm_clf.predict(X_val)\n",
    "svm_acc = accuracy_score(y_val, svm_pred)\n",
    "print(f\"SVM Accuracy: {svm_acc}\")\n",
    "\n",
    "# Example Logistic Regression training\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_val)\n",
    "lr_acc = accuracy_score(y_val, lr_pred)\n",
    "print(f\"Logistic Regression Accuracy: {lr_acc}\")\n",
    "\n",
    "# Example Random Forest training\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_val)\n",
    "rf_acc = accuracy_score(y_val, rf_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVMmodel/SVM_model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC  # for classification tasks\n",
    "from sklearn.svm import SVR  # for regression tasks\n",
    "from joblib import dump, load\n",
    "\n",
    "# Example: Train a Random Forest model\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Assuming X_train and y_train are your training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "model_path = 'SVMmodel/SVM_model.joblib'\n",
    "\n",
    "# Save the model to the specified path\n",
    "dump(svm_clf, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "loaded_model = load(model_path)\n",
    "\n",
    "# Now you can use loaded_model for predictions or further operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "\n",
    "pyautogui.FAILSAFE = True  # Enable fail-safe (default behavior)\n",
    "pyautogui.FAILSAFE_INTERVAL = 2.0  # Adjust the fail-safe check interval if needed\n",
    "pyautogui.PAUSE = 0.1  # Adjust the pause duration between PyAutoGUI actions if needed\n",
    "pyautogui.FAILSAFE_POINTS = [(0, 0)]  # Adjust the fail-safe corner points to ignore\n",
    "\n",
    "# Adjust the distance before the fail-safe triggers\n",
    "pyautogui.FAILSAFE_MARGIN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyautogui\n",
    "from joblib import load  # Import load function from joblib\n",
    "\n",
    "pyautogui.FAILSAFE = False  # Disable PyAutoGUI fail-safe\n",
    "\n",
    "def extract_eye_region(frame):\n",
    "    # Implement eye region extraction here\n",
    "    # Example: Using Haar cascades for eye detection\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    eyes = eye_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if len(eyes) > 0:\n",
    "        (ex, ey, ew, eh) = eyes[0]\n",
    "        eye_region = frame[ey:ey+eh, ex:ex+ew]\n",
    "        return eye_region\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def predict_direction(frame, model):\n",
    "    # Extract features from the eye region (assuming your feature extraction method)\n",
    "    features = extract_features(frame)\n",
    "    \n",
    "    if features is not None:\n",
    "        # Predict direction using the trained model\n",
    "        prediction = model.predict([features])[0]\n",
    "        \n",
    "        # Map integer prediction to direction label\n",
    "        direction_labels = {0: 'down',1: 'left', 2: 'right', 3: 'up'}\n",
    "        predicted_direction = direction_labels.get(prediction, None)\n",
    "        \n",
    "        return predicted_direction\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_features(frame):\n",
    "    # Implement feature extraction here\n",
    "    # Example: Convert to grayscale and resize to (32, 32)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (32, 32))\n",
    "    flattened = resized.flatten()\n",
    "    return flattened\n",
    "\n",
    "def move_cursor_with_eye_direction(video_path, model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    screen_width, screen_height = pyautogui.size()\n",
    "    cursor_step = 50  # Number of pixels to move the cursor\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process frame to extract eye region\n",
    "        eye_region = extract_eye_region(frame)\n",
    "        \n",
    "        if eye_region is not None:\n",
    "            # Predict direction\n",
    "            direction = predict_direction(eye_region, model)\n",
    "            \n",
    "            # Assuming 'direction' is the variable holding the predicted direction\n",
    "            if direction is not None:\n",
    "                # Move cursor based on predicted direction\n",
    "                if direction == 'up':\n",
    "                    pyautogui.moveRel(0, -cursor_step)  # Move cursor up by cursor_step pixels\n",
    "                elif direction == 'down':\n",
    "                    pyautogui.moveRel(0, cursor_step)  # Move cursor down by cursor_step pixels\n",
    "                elif direction == 'left':\n",
    "                    pyautogui.moveRel(-cursor_step, 0)  # Move cursor left by cursor_step pixels\n",
    "                elif direction == 'right':\n",
    "                    pyautogui.moveRel(cursor_step, 0)  # Move cursor right by cursor_step pixels\n",
    "                \n",
    "                # Display direction text on the frame\n",
    "                cv2.putText(frame, f\"Direction: {direction}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "        \n",
    "        # Press 'q' on keyboard to exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release video capture and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage with your video file\n",
    "video_path = r'video\\WhatsApp Video 2024-07-31 at 23.12.48_43b230b9.mp4'\n",
    "\n",
    "# Load your trained SVM model\n",
    "model_path = r'SVMmodel\\SVM_model.joblib'\n",
    "svm_clf = load(model_path)  # Load your trained model file using joblib\n",
    "\n",
    "# Call the function to move cursor based on eye direction in real-time\n",
    "move_cursor_with_eye_direction(video_path, svm_clf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
